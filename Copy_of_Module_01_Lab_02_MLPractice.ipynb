{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harikavelaga9999/FMML-LAB1/blob/main/Copy_of_Module_01_Lab_02_MLPractice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Eu9VZbF01eq"
      },
      "source": [
        "# Machine learning terms and metrics\n",
        "\n",
        "FMML Module 1, Lab 2<br>\n",
        "\n",
        "\n",
        " In this lab, we will show a part of the ML pipeline by extracting features, training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qBvyEem0vLi"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "# set randomseed\n",
        "rng = np.random.default_rng(seed=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3t59g5s1HfC"
      },
      "source": [
        "In this lab, we will use the California Housing dataset. There are 20640 samples, each with 8 attributes like income of the block, age of the houses per district etc. The task is to predict the cost of the houses per district.\n",
        "\n",
        "Let us download and examine the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LpqjN991GGJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cbd62a7-39a3-41d7-c07b-073b50dd02b0"
      },
      "source": [
        " dataset =  datasets.fetch_california_housing()\n",
        " # print(dataset.DESCR)  # uncomment this if you want to know more about this dataset\n",
        " # print(dataset.keys())  # if you want to know what else is there in this dataset\n",
        " dataset.target = dataset.target.astype(np.int) # so that we can classify\n",
        " print(dataset.data.shape)\n",
        " print(dataset.target.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20640, 8)\n",
            "(20640,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-60ae2e9a125e>:4: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  dataset.target = dataset.target.astype(np.int) # so that we can classify\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNx4174W5xRg"
      },
      "source": [
        "Here is a function for calculating the 1-nearest neighbours"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07zpydQj1hIQ"
      },
      "source": [
        "def NN1(traindata, trainlabel, query):\n",
        "  diff  = traindata - query  # find the difference between features. Numpy automatically takes care of the size here\n",
        "  sq = diff*diff # square the differences\n",
        "  dist = sq.sum(1) # add up the squares\n",
        "  label = trainlabel[np.argmin(dist)] # our predicted label is the label of the training data which has the least distance from the query\n",
        "  return label\n",
        "\n",
        "def NN(traindata, trainlabel, testdata):\n",
        "  # we will run nearest neighbour for each sample in the test data\n",
        "  # and collect the predicted classes in an array using list comprehension\n",
        "  predlabel = np.array([NN1(traindata, trainlabel, i) for i in testdata])\n",
        "  return predlabel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03JktkfIGaje"
      },
      "source": [
        "We will also define a 'random classifier', which randomly allots labels to each sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fogWAtjyGhAH"
      },
      "source": [
        "def RandomClassifier(traindata, trainlabel, testdata):\n",
        "  # in reality, we don't need these arguments\n",
        "\n",
        "  classes = np.unique(trainlabel)\n",
        "  rints = rng.integers(low=0, high=len(classes), size=len(testdata))\n",
        "  predlabel = classes[rints]\n",
        "  return predlabel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hjf1KHs7fU5"
      },
      "source": [
        "Let us define a metric 'Accuracy' to see how good our learning algorithm is. Accuracy is the ratio of the number of correctly classified samples to the total number of samples. The higher the accuracy, the better the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouuCqWU07bz-"
      },
      "source": [
        "def Accuracy(gtlabel, predlabel):\n",
        "  assert len(gtlabel)==len(predlabel), \"Length of the groundtruth labels and predicted labels should be the same\"\n",
        "  correct = (gtlabel==predlabel).sum() # count the number of times the groundtruth label is equal to the predicted label.\n",
        "  return correct/len(gtlabel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vJFwBFa9Klw"
      },
      "source": [
        "Let us make a function to split the dataset with the desired probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko0VzpSM2Tdi"
      },
      "source": [
        "def split(data, label, percent):\n",
        "  # generate a random number for each sample\n",
        "  rnd = rng.random(len(label))\n",
        "  split1 = rnd<percent\n",
        "  split2 = rnd>=percent\n",
        "  split1data = data[split1,:]\n",
        "  split1label = label[split1]\n",
        "  split2data = data[split2,:]\n",
        "  split2label = label[split2]\n",
        "  return split1data, split1label, split2data, split2label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcK3LEAJ_LGC"
      },
      "source": [
        "We will reserve 20% of our dataset as the test set. We will not change this portion throughout our experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBZkHBLJ1iU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "580340d7-6e25-495d-c97b-0faee8298bfc"
      },
      "source": [
        "testdata, testlabel, alltraindata, alltrainlabel = split(dataset.data, dataset.target, 20/100)\n",
        "print('Number of test samples = ', len(testlabel))\n",
        "print('Number of other samples = ', len(alltrainlabel))\n",
        "print('Percent of test data = ', len(testlabel)*100/len(dataset.target),'%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test samples =  4144\n",
            "Number of other samples =  16496\n",
            "Percent of test data =  20.07751937984496 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6Ss0Z6IAGNV"
      },
      "source": [
        "## Experiments with splits\n",
        "\n",
        "Let us reserve some of our train data as a validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFew2iry_7W7"
      },
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 75/100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60hiu4clFN1i"
      },
      "source": [
        "What is the accuracy of our classifiers on the train dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBlZDTHUFTZx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1d60c51-bc48-4128-e0fd-847617c5e612"
      },
      "source": [
        "trainpred = NN(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Train accuracy using nearest neighbour is \", trainAccuracy)\n",
        "\n",
        "trainpred = RandomClassifier(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Train accuracy using random classifier is \", trainAccuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy using nearest neighbour is  1.0\n",
            "Train accuracy using random classifier is  0.164375808538163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h08-9gJDtSy"
      },
      "source": [
        "For nearest neighbour, the train accuracy is always 1. The accuracy of the random classifier is close to 1/(number of classes) which is 0.1666 in our case.\n",
        "\n",
        "Let us predict the labels for our validation set and get the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h7bXoW_2H3v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c766c434-dd09-4e9f-da84-aa4ef0c5595a"
      },
      "source": [
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using nearest neighbour is \", valAccuracy)\n",
        "\n",
        "valpred = RandomClassifier(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using random classifier is \", valAccuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy using nearest neighbour is  0.34108527131782945\n",
            "Validation accuracy using random classifier is  0.1688468992248062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py9bLguFEjfg"
      },
      "source": [
        "Validation accuracy of nearest neighbour is considerably less than its train accuracy while the validation accuracy of random classifier is the same. However, the validation accuracy of nearest neighbour is twice that of the random classifier.\n",
        "\n",
        "Now let us try another random split and check the validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujm3cyYzEntE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8d9a355-4f59-4da0-8242-bddebe294fcc"
      },
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 75/100)\n",
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy of nearest neighbour is \", valAccuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy of nearest neighbour is  0.34048257372654156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSOx7U83EKie"
      },
      "source": [
        "You can run the above cell multiple times to try with different random splits.\n",
        "We notice that the accuracy is different for each run, but close together.\n",
        "\n",
        "Now let us compare it with the accuracy we get on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNEZ5ToYBEDW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2d8b5d9-7805-43ff-a62d-7558eaee091a"
      },
      "source": [
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "testAccuracy = Accuracy(testlabel, testpred)\n",
        "print('Test accuracy is ', testAccuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy is  0.34917953667953666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3dGD531K3gH"
      },
      "source": [
        "### Try it out for yourself and answer:\n",
        "1. How is the accuracy of the validation set affected if we increase the percentage of validation set? What happens when we reduce it?\n",
        "2. How does the size of the train and validation set affect how well we can predict the accuracy on the test set using the validation set?\n",
        "3. What do you think is a good percentage to reserve for the validation set so that thest two factors are balanced?\n",
        "\n",
        "Answer for both nearest neighbour and random classifier. You can note down the values for your experiments and plot a graph using  <a href=https://matplotlib.org/stable/gallery/lines_bars_and_markers/step_demo.html#sphx-glr-gallery-lines-bars-and-markers-step-demo-py>plt.plot<href>. Check also for extreme values for splits, like 99.9% or 0.1%"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1ST QUESTION ANSWER **"
      ],
      "metadata": {
        "id": "74AniboH8bSd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=>Higher Validation Accuracy: When you allocate a larger percentage of your data to the validation set, it usually results in a smaller training set. This can lead to better estimates of validation accuracy because your model is evaluated on a larger and more representative subset of the data. It helps in assessing how well your model generalizes to unseen data.\n",
        "\n",
        "=>Risk of Underfitting: However, increasing the percentage of the validation set also reduces the amount of data available for training. If you make the validation set too large, your model might not have enough training data to learn meaningful patterns, leading to underfitting.\n",
        "\n",
        "Reducing the Percentage of Validation Set:\n",
        "\n",
        "=>Higher Training Accuracy: Reducing the percentage of the validation set increases the size of the training set, which can allow your model to learn from more data. This can lead to higher training accuracy.\n",
        "\n",
        "=>Risk of Overfitting: While increasing the training set size can improve training accuracy, it can also make your model more prone to overfitting. With a smaller validation set, you have less data to evaluate how well your model generalizes, which may result in overly optimistic estimates of performance.\n",
        "\n",
        "*The choice of the percentage allocated to the validation set depends on several factors:\n",
        "\n",
        "=>Size of the Dataset: In small datasets, you might need to allocate a larger portion to the validation set to ensure a representative sample for evaluation.\n",
        "\n",
        "=>Model Complexity: More complex models often require larger validation sets to assess their performance accurately.\n",
        "\n",
        "=>Computational Resources: A larger validation set might require more time and computational resources for training and evaluation.\n",
        "\n",
        "=>Overfitting Concerns: If you suspect overfitting, a larger validation set can help detect it, but it might also exacerbate the overfitting problem during training.\n",
        "\n",
        "=>In practice, a common practice is to use a 70-30 or 80-20 split for training and validation data. However, these percentages can vary depending on the specific problem and dataset characteristics. It's often necessary to experiment with different splits to find the balance that works best for your particular situation. Cross-validation can also be a valuable technique to assess the model's performance more robustly when dealing with different data splits.\n",
        "\n"
      ],
      "metadata": {
        "id": "pGwPk68t8iOg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2ND QUESTION ANSWER **"
      ],
      "metadata": {
        "id": "oc-Vrc2S8q4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">The size of the training and validation sets can have an impact on how well you can predict the accuracy on the test set using the validation set. Here's how different scenarios may play out:\n",
        "\n",
        "Adequate Training Data and Representative Validation Set:\n",
        "Scenario: You have a reasonably large training set, and the validation set is representative of the data distribution. The validation set is neither too small nor too large compared to the training set.\n",
        "\n",
        "Effect: In this scenario, you are likely to get a good estimate of your model's performance on the test set. The validation set is representative enough to provide an accurate assessment of how well your model generalizes to unseen data.\n",
        "\n",
        "Inadequate Training Data:\n",
        "\n",
        "Scenario: If your training set is very small compared to the validation set, your model may not have enough data to learn meaningful patterns, and its performance on the validation set may not be a reliable indicator of test set performance.\n",
        "\n",
        "Effect: In this case, the validation accuracy may overestimate the model's true performance on the test set because the model hasn't been adequately trained. Your model may not generalize well when faced with unseen data.\n",
        "\n",
        "Inadequate Validation Set:\n",
        "\n",
        "Scenario: If the validation set is too small compared to the training set, your model may not be thoroughly evaluated for generalization, and its performance on the validation set may not accurately predict test set performance.\n",
        "\n",
        "Effect: Here, the validation accuracy may underestimate the model's true performance on the test set. The small validation set may not capture the variability in the data, and the estimate of model performance may not be as reliable.\n",
        "\n",
        "Representative Validation Set but Overfitting:\n",
        "\n",
        "Scenario: Your training set is adequate in size, but the model is overfitting the training data, resulting in a high validation accuracy.\n",
        "\n",
        "Effect: In this case, the validation accuracy may not accurately predict test set performance because the model has memorized the training data but hasn't learned to generalize well. Test accuracy could be lower than the validation accuracy due to overfitting.\n",
        "\n",
        "In summary, the size of the training and validation sets is crucial in assessing how well the validation set predicts test set accuracy. It's essential to strike a balance between the sizes of these sets to ensure that the validation set is representative and that the model has been adequately trained. Cross-validation can be a useful technique to mitigate some of these issues by repeatedly splitting the data into training and validation subsets and assessing performance across multiple iterations, providing a more robust estimate of test set accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "umgzqxQf8wHy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** 3RD QUESTION ANSWER**"
      ],
      "metadata": {
        "id": "wMxT2YO089qp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=>The percentage of data to reserve for the validation set can vary depending on the size of your dataset and the specific problem you're tackling. However, a commonly used guideline is to reserve around 20% to 30% of your data for the validation set when you have a reasonably sized dataset (i.e., not extremely small). This range often strikes a good balance between having enough data for training and a representative validation set while avoiding overfitting and underfitting issues.\n",
        "\n",
        "*Here are some considerations when choosing the percentage for the validation set:\n",
        "\n",
        "*Dataset Size: If you have a very small dataset, you might need to allocate a larger portion to the validation set (e.g., 40% or even 50%) to ensure that the validation set remains reasonably sized. In such cases, you might also consider techniques like k-fold cross-validation to make the most of your limited data.\n",
        "\n",
        "*Model Complexity: More complex models often require larger validation sets to assess their performance accurately. If you're working with a simple model, a smaller validation set might suffice.\n",
        "\n",
        "*Computational Resources: Keep in mind the computational resources available to you. A larger validation set can increase the time and computational power required for training and evaluation.\n",
        "\n",
        "=>Overfitting Concerns: If you suspect overfitting, a larger validation set can help detect it. However, if the training dataset is too small, overfitting may still be a concern, and you should focus on techniques to mitigate it, such as regularization.\n",
        "\n",
        "=>Data Variability: Consider the variability in your dataset. If your data has significant variations or is prone to outliers, a larger validation set can help capture these variations and provide a more robust estimate of model performance.\n",
        "\n",
        "=>It's important to note that there is no one-size-fits-all answer, and the choice of the validation set size may require experimentation and fine-tuning based on your specific problem and dataset characteristics. Additionally, cross-validation techniques, such as k-fold cross-validation, can be used to assess model performance more robustly when dealing with different data splits, which can help in situations where it's challenging to determine the ideal validation set size.\n",
        "\n"
      ],
      "metadata": {
        "id": "jydBxBTp9CmO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnYvkAZLQY7h"
      },
      "source": [
        "## Multiple Splits\n",
        "\n",
        "One way to get more accurate estimates for the test accuracy is by using <b>crossvalidation</b>. Here, we will try a simple version, where we do multiple train/val splits and take the average of validation accuracies as the test accuracy estimation. Here is a function for doing this. Note that this function will take a long time to execute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4nGCUQXBTzo"
      },
      "source": [
        "# you can use this function for random classifier also\n",
        "def AverageAccuracy(alldata, alllabel, splitpercent, iterations, classifier=NN):\n",
        "  accuracy = 0\n",
        "  for ii in range(iterations):\n",
        "    traindata, trainlabel, valdata, vallabel = split(alldata, alllabel, splitpercent)\n",
        "    valpred = classifier(traindata, trainlabel, valdata)\n",
        "    accuracy += Accuracy(vallabel, valpred)\n",
        "  return accuracy/iterations # average of all accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3qtNar7Bbik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10efc4d8-2f31-4d68-f34a-62e4d7a29bb9"
      },
      "source": [
        "print('Average validation accuracy is ', AverageAccuracy(alltraindata, alltrainlabel, 75/100, 10, classifier=NN))\n",
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "print('test accuracy is ',Accuracy(testlabel, testpred) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average validation accuracy is  0.33584635395170215\n",
            "test accuracy is  0.34917953667953666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33GIn4x5VH-d"
      },
      "source": [
        "This is a very simple way of doing cross-validation. There are many well-known algorithms for cross-validation, like k-fold cross-validation, leave-one-out etc. This will be covered in detail in a later module. For more information about cross-validation, check <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>Cross-validatioin (Wikipedia)</a>\n",
        "\n",
        "### Questions\n",
        "1. Does averaging the validation accuracy across multiple splits give more consistent results?\n",
        "2. Does it give more accurate estimate of test accuracy?\n",
        "3. What is the effect of the number of iterations on the estimate? Do we get a better estimate with higher iterations?\n",
        "4. Consider the results you got for the previous questions. Can we deal with a very small train dataset or validation dataset by increasing the iterations?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1ST QUESTION ANSWER **"
      ],
      "metadata": {
        "id": "SdcFdqGd9aLX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=>Yes,i'm agree with the question, averaging the validation accuracy across multiple splits (commonly referred to as \"cross-validation\") can indeed provide more consistent and reliable results when evaluating the performance of a machine learning model. Cross-validation is a robust technique used to assess how well a model generalizes to unseen data and can help mitigate the impact of data randomness and variability.\n",
        "\n",
        "Here's how cross-validation works and why it's beneficial:\n",
        "\n",
        "=>Reduced Variance: When you split your dataset into multiple subsets (folds) and train/validate your model on each of them separately, you get multiple accuracy scores. Averaging these scores reduces the impact of outliers or specific quirks in a single data split, which can lead to a more stable and reliable estimate of your model's performance.\n",
        "\n",
        "=>Better Generalization Assessment: Cross-validation provides a more comprehensive assessment of your model's generalization ability. By testing your model on different subsets of the data, you gain insights into how well it performs across various data samples, which can help you detect overfitting or underfitting issues.\n",
        "\n",
        "=>Maximizing Data Utilization: In traditional train-test splitting, you might reserve a significant portion of your data for testing, which reduces the amount of data available for training. Cross-validation iteratively uses different subsets for validation while maximizing the use of your data for training, which can lead to more efficient model training.\n",
        "\n",
        "=>Common cross-validation techniques include k-fold cross-validation (where you split your data into k subsets and use each as a validation set while the rest serve as the training set) and stratified cross-validation (ensuring that each fold has a similar class distribution as the original data).\n",
        "\n",
        "=>In summary, averaging the validation accuracy across multiple splits through cross-validation is a best practice for model evaluation. It provides a more reliable estimate of your model's performance and helps you make better decisions about model selection and hyperparameter tuning.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gq69X35X9epi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2ND QUESTION ANSWER **"
      ],
      "metadata": {
        "id": "wBJgFkv19kcJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-validation, while a valuable technique for estimating the performance of a machine learning model, does not directly provide a more accurate estimate of the model's test accuracy. Instead, it gives you a more accurate estimate of how well your model is likely to perform on unseen data based on the available training data. Here's why:\n",
        "\n",
        "*Validation vs. Test Data: In\n",
        "cross-validation, you split your dataset into multiple subsets and use them alternately for training and validation. These validation subsets are still part of your original dataset. They are used to estimate how well your model generalizes to unseen data but are not entirely independent of your training data.\n",
        "\n",
        "*Test Data Independence: The true test accuracy, on the other hand, is determined by using a separate and entirely independent dataset that the model has never seen during training or validation. This is often referred to as the \"holdout\" test set, and it simulates how your model will perform when applied to completely new and unseen data.\n",
        "\n",
        "*While cross-validation provides a more robust estimate of your model's performance on the data you have, it doesn't replace the need for a holdout test set to assess how your model will perform in real-world scenarios. The holdout test set is crucial for understanding how your model will generalize to new, previously unseen data.\n",
        "\n",
        "In practice, the workflow typically involves:\n",
        "\n",
        "=>Cross-validation: Use cross-validation to assess your model's performance on the available data, refine hyperparameters, and make model selection decisions.\n",
        "\n",
        "=>Holdout Test Set: After you've finalized your model, evaluate it on a holdout test set to get an accurate estimate of how it will perform in real-world applications. This gives you a more accurate measure of the model's test accuracy.\n",
        "\n",
        "*In summary, cross-validation provides a useful estimate of model performance on your available data but does not directly replace the need for a holdout test set to assess performance on truly independent, unseen data. Both techniques are important for a comprehensive evaluation of your machine learning model.\n",
        "\n"
      ],
      "metadata": {
        "id": "VPoLEHJy9p2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** 3RD QUESTION ANSWER**"
      ],
      "metadata": {
        "id": "-iFFUeE49uo_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">In the context of cross-validation, the number of iterations refers to the number of times you perform the cross-validation process, where you split your dataset into different subsets for training and validation. This is often denoted as \"k\" in k-fold cross-validation. Each iteration gives you an estimate of model performance.\n",
        "\n",
        "*The effect of the number of iterations on the estimate is as follows:\n",
        "\n",
        "=>Higher Iterations (Higher k): Using a higher number of iterations or folds in cross-validation (e.g., 10-fold or 5-fold) can provide a more stable and reliable estimate of your model's performance. It reduces the impact of randomness in the data splitting process. With more iterations, you are essentially averaging the results from multiple train-validation splits, which can lead to a more robust estimate.\n",
        "\n",
        "=>Lower Iterations (Lower k): Using a lower number of iterations (e.g., 2-fold or 3-fold) can be computationally less expensive, but it might result in more variability in your performance estimates. The estimates can be sensitive to the specific data split in each iteration, and you may get less stable results.\n",
        "\n",
        "=>However, it's essential to strike a balance between computational cost and accuracy when choosing the number of iterations. Very high values of k (e.g., leave-one-out cross-validation with k equal to the number of data points) can be computationally expensive and may not significantly improve the estimate's stability. On the other hand, very low values of k may not provide a reliable estimate due to increased variability.\n",
        "\n",
        "=>In practice, common choices for k in\n",
        "k-fold cross-validation are 5 or 10, as they strike a reasonable balance between stability and computational cost. The choice of the number of iterations should depend on the size of your dataset, available computational resources, and the need for a stable estimate.\n",
        "\n",
        "*In summary, increasing the number of iterations in cross-validation can lead to a more stable and reliable estimate of your model's performance, but there is a trade-off with computational cost. The optimal value of k depends on various factors and should be chosen carefully based on your specific situation.\n",
        "\n"
      ],
      "metadata": {
        "id": "VVtfRtdE9y8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4TH QUESTION ANSWER **"
      ],
      "metadata": {
        "id": "YmnuUiZB94vT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=>Increasing the number of iterations (folds) in cross-validation can help when dealing with a very small training dataset or validation dataset to some extent, but it has limitations. While more iterations can provide a more stable estimate of model performance, they do not fundamentally address the issues associated with having a very small dataset.\n",
        "\n",
        "Here are some considerations:\n",
        "\n",
        "*Advantages of Increasing Iterations:\n",
        "\n",
        "=>Improved Stability: With more iterations, you reduce the impact of random variations in the data splitting process. This can lead to more consistent estimates of model performance.\n",
        "\n",
        "=>Better Utilization: When you have a small dataset, using more iterations can help you make better use of the limited data you have for both training and validation.\n",
        "\n",
        "*Limitations and Considerations:\n",
        "\n",
        "=>Data Size: The fundamental limitation of having a small dataset is that your model may not be able to learn meaningful patterns, and the performance estimates may not generalize well to new data. Increasing iterations cannot create more data; it can only provide more robust estimates based on the available data.\n",
        "\n",
        "=>Overfitting Risk: With very small datasets, you should be cautious about overfitting. If your dataset is extremely small, even a portion of it used for validation in each fold may be significant. This can lead to overly optimistic estimates of model performance. In such cases, it might be better to use techniques like leave-one-out cross-validation or stratified sampling to ensure each fold has a representative distribution.\n",
        "\n",
        "=>Computational Cost: Increasing the number of iterations can be computationally expensive, especially if your dataset is small, to begin with. You should consider the trade-off between computational resources and the benefit gained from additional iterations.\n",
        "\n",
        "=>In summary, while increasing the number of iterations in cross-validation can help mitigate some of the challenges associated with very small datasets, it does not solve the fundamental problem of limited data. You should be cautious about overfitting and carefully consider the computational cost when deciding on the number of iterations. Additionally, other techniques such as data augmentation, transfer learning, or obtaining more data, if possible, may be more effective ways to address the challenges of a small dataset."
      ],
      "metadata": {
        "id": "UbNsrkQd9-bW"
      }
    }
  ]
}